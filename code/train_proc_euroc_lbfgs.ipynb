{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "## 本算法使用CPU训练\n",
    "## 此条命令运行必须放在 import torch 之前，否则不能生效\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" \n",
    "import sys\n",
    "### pytorch 相关\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "### pytorch 自定义损失函数\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "\n",
    "### torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import quaternion # qw qx qy qz\n",
    "\n",
    "### matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "### numpy\n",
    "import numpy as np\n",
    "\n",
    "### time，延时函数-秒 time.sleep(1) 延时1秒\n",
    "import time \n",
    "\n",
    "### PIL\n",
    "from PIL import Image\n",
    "\n",
    "### SciPy\n",
    "import scipy.signal as signal\n",
    "\n",
    "# jupyter 使用 matplotlib 绘图所需，否则会挂掉\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# 数据集相关\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 配置文件读取 yaml\n",
    "import yaml\n",
    "\n",
    "# 进度条\n",
    "import tqdm\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tinyGC2L import TinyGC2L_Net\n",
    "from myDataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_path = '400_samples'\n",
    "train_data_path = ['400_total']\n",
    "train_data_path = [f'{data_base_path}/{d}' for d in train_data_path]\n",
    "train_epoch_cnt = 10\n",
    "device = torch.device('cuda:0') # cuda:0 cpu\n",
    "\n",
    "weight_dir = '../weights/weights_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_function, data_loader, device, epoch):\n",
    "    # 在训练开始之前写上 model.trian() ，在测试时写上 model.eval() \n",
    "    # 避免 BN层 和 Dropou 的影响\n",
    "    model.eval()\n",
    "    # 累积损失\n",
    "    accu_loss = torch.zeros(1).to(device)\n",
    "    for step, (data, file_path) in enumerate(data_loader, start=0):\n",
    "    # for step, data in enumerate(data_loader, start=0):\n",
    "        timestampns_set = data['timestampns_set'] # [1000, 226]\n",
    "        gyro_set = data['gyro_set'] # [1000, 226, 3]  [​​batch_size​​​,​​channel​​​,​​height​​​,​​width​​]\n",
    "        start_quat = data['start_quat'] # [1000, 4]\n",
    "        label = data['label']\n",
    "\n",
    "        # 正向传播\n",
    "        output = model(timestampns_set.to(device), gyro_set.to(device), start_quat.to(device))\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = loss_function(output, label.to(device))\n",
    "\n",
    "        # 打印统计信息\n",
    "        accu_loss += loss.detach()\n",
    "\n",
    "        debug_desc = \"[train epoch {}] evaluation loss: {:.6f}\".format(epoch, accu_loss.item() / (step + 1))\n",
    "        # 判断是否出现无效 loss 值\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "        #for循环结束后，统计每个 step 的 loss\n",
    "        # return accu_loss.item() / (step + 1)\n",
    "        return debug_desc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train set size: 261\n",
      "Total val set size: 52\n",
      "Create new best-model.pth\n",
      "[train epoch 0] evaluation loss: 0.995606\n",
      "[train epoch 1] evaluation loss: 0.216183\n",
      "best-model.pth's train loss 0.216183\n",
      "[train epoch 2] evaluation loss: 0.033010\n",
      "best-model.pth's train loss 0.03301\n",
      "[train epoch 3] evaluation loss: 0.005581\n",
      "best-model.pth's train loss 0.005581\n",
      "[train epoch 4] evaluation loss: 0.001136\n",
      "best-model.pth's train loss 0.001136\n",
      "[train epoch 5] evaluation loss: 0.000485\n",
      "best-model.pth's train loss 0.000485\n",
      "[train epoch 6] evaluation loss: 0.000286\n",
      "best-model.pth's train loss 0.000286\n",
      "[train epoch 7] evaluation loss: 0.000227\n",
      "best-model.pth's train loss 0.000227\n",
      "[train epoch 8] evaluation loss: 0.000206\n",
      "best-model.pth's train loss 0.000206\n",
      "[train epoch 9] evaluation loss: 0.000200\n",
      "best-model.pth's train loss 0.0002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for data_path in train_data_path:\n",
    "    save_dir = weight_dir + data_path\n",
    "\n",
    "    # train_dataset = MyDataset(dataset_name =  'Euroc/Len_400_Samples', # 'simulator',\n",
    "    train_dataset = MyDataset(dataset_name =  data_path, # 'simulator',\n",
    "                            category = 'train',\n",
    "                            transform = None)\n",
    "\n",
    "    # val_dataset = MyDataset(dataset_name = 'Euroc/Len_400_Samples', # 'simulator',\n",
    "    val_dataset = MyDataset(dataset_name = data_path, # 'simulator',\n",
    "                            category = 'val',\n",
    "                            transform = None)\n",
    "\n",
    "    print('Total train set size: {}'.format(train_dataset.__len__()))\n",
    "    print('Total val set size: {}'.format(val_dataset.__len__()))\n",
    "\n",
    "    # 使用DataLoader可以利用多线程，batch,shuffle等\n",
    "    trainset_dataloader = DataLoader(dataset = train_dataset,\n",
    "                                    batch_size = 150, # 50\n",
    "                                    shuffle = True,\n",
    "                                    num_workers = 5)\n",
    "\n",
    "    val_dataloader = DataLoader(dataset = val_dataset,\n",
    "                            batch_size = val_dataset.__len__(), # 30  \n",
    "                            shuffle = False, # 训练集往往需要进行shuffle，这样可以避免对顺序的依赖，而验证集不需要\n",
    "                            num_workers = 5)\n",
    "\n",
    "    ## 设定训练参数\n",
    "    ddi_net = TinyGC2L_Net(device)\n",
    "    ddi_net = ddi_net.to(device)\n",
    "\n",
    "    # 判断 save_dir 路径中是否存在　best-model.pth 文件\n",
    "    weight_file_path = os.path.join(save_dir, 'best-model.pth')  # 替换为你想要检查的文件路径\n",
    "\n",
    "    if os.path.exists(weight_file_path):\n",
    "        ddi_net.load_state_dict(torch.load(weight_file_path))\n",
    "    else:\n",
    "        print('Create new best-model.pth')\n",
    "\n",
    "    # 定义损失函数\n",
    "    loss_function = torch.nn.MSELoss(size_average=None, reduce=None, reduction='sum')  # reduction='sum'\n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.LBFGS(ddi_net.parameters(), lr=0.05)\n",
    "    # 定义动态学习率\n",
    "    lr_step = StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "\n",
    "    # 训练过程\n",
    "    # 一个epoch即对整个训练集进行一次训练，循环几次就是几轮\n",
    "    last_loss = 0\n",
    "    os.makedirs(save_dir, exist_ok=True)  # 创建多级路径，如果已存在则不会报错\n",
    "    for epoch in range(train_epoch_cnt):\n",
    "        # train\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            for step, (data, file_path) in enumerate(trainset_dataloader, start=0):\n",
    "            # for step, data in enumerate(trainset_dataloader, start=0):\n",
    "                timestampns_set = data['timestampns_set'] # [1000, 226]\n",
    "                gyro_set = data['gyro_set'] # [1000, 226, 3]  [​​batch_size​​​,​​channel​​​,​​height​​​,​​width​​]\n",
    "                start_quat = data['start_quat'] # [1000, 4]\n",
    "                label = data['label']\n",
    "                 # 正向传播\n",
    "                timestampns_set = timestampns_set.to(device)\n",
    "                gyro_set = gyro_set.to(device)\n",
    "                start_quat = start_quat.to(device)\n",
    "                output = ddi_net(timestampns_set, gyro_set, start_quat)\n",
    "                # 计算损失\n",
    "                loss = loss_function(output, label.to(device))\n",
    "                 # 反向传播\n",
    "                loss.backward()\n",
    "            # 输出训练损失\n",
    "            # print(f'Epoch {epoch+1}/{train_epoch_cnt}, Loss: {loss.item()}')\n",
    "            return loss.item()\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "        # 学习率更新\n",
    "        lr_step.step()\n",
    "        eval_debug_desc = evaluate(\n",
    "                                    model=ddi_net,\n",
    "                                    loss_function=loss_function,\n",
    "                                    data_loader=val_dataloader,\n",
    "                                    device=device,\n",
    "                                    epoch=epoch)\n",
    "\n",
    "        print(eval_debug_desc)\n",
    "\n",
    "        with open(os.path.join(save_dir, 'train_eval_loss.txt'), 'a') as file: \n",
    "            file.write('{}\\n'.format(eval_debug_desc))\n",
    "\n",
    "        # 保存训练损失更小的权重参数\n",
    "        current_eval_loss = float(re.findall(r'\\d+\\.\\d+|\\d+', eval_debug_desc)[1])\n",
    "        if epoch == 0:\n",
    "            last_loss = current_eval_loss\n",
    "        elif current_eval_loss < last_loss:\n",
    "            save_path = os.path.join(save_dir, \"best-model.pth\".format(epoch))\n",
    "            torch.save(ddi_net.state_dict(), save_path)\n",
    "            print(\"best-model.pth's train loss {}\".format(current_eval_loss))\n",
    "            last_loss = current_eval_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lwgc-net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
